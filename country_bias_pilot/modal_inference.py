"""Modal-based remote GPU inference for country bias experiments.

Usage:
    # Run all models
    modal run modal_inference.py

    # Run specific models
    modal run modal_inference.py --models llama3-8b mistral-7b

    # Smoke test (1 model, 2 pairs, 1 scenario)
    modal run modal_inference.py --test

    # List available models
    modal run modal_inference.py --list-models
"""

from __future__ import annotations

import json
import os
from pathlib import Path

import modal

# ---------------------------------------------------------------------------
# Modal setup
# ---------------------------------------------------------------------------

app = modal.App("llm-country-bias")

# Volume to cache HuggingFace model weights across runs
model_cache = modal.Volume.from_name("hf-model-cache", create_if_missing=True)

# Container image with all dependencies
image = (
    modal.Image.debian_slim(python_version="3.11")
    .pip_install(
        "torch",
        "transformers",
        "accelerate",
        "huggingface_hub",
        "tqdm",
        "pandas",
        "numpy",
    )
)

# ---------------------------------------------------------------------------
# Config (duplicated from config.py to avoid import issues in Modal)
# ---------------------------------------------------------------------------

DTYPE = "float32"
TOP_K_CHECK = 20
COMPLIANCE_WARN = 0.5

MODELS = {
    "llama3-8b": "meta-llama/Meta-Llama-3-8B",
    "qwen2.5-7b": "Qwen/Qwen2.5-7B",
    "mistral-7b": "mistralai/Mistral-7B-v0.3",
    "deepseek-v2-lite": "deepseek-ai/DeepSeek-V2-Lite",
    "falcon-7b": "tiiuae/falcon-7b",
    "gemma2-9b": "google/gemma-2-9b",
    "gpt-oss-20b": "openai-community/gpt-oss-20b",
}

FICTIONAL_PAIRS = [
    ("Aeloria", "Brevnia"),
    ("Zembala", "Kotundi"),
    ("Junwei", "Khemara"),
    ("Aeloria", "Zembala"),
    ("Aeloria", "Junwei"),
    ("Zembala", "Junwei"),
]

REAL_PAIRS = [
    ("UK", "France"),
    ("Germany", "Japan"),
    ("UK", "India"),
    ("Germany", "South Korea"),
    ("Japan", "China"),
    ("South Korea", "Saudi Arabia"),
    ("India", "Russia"),
    ("Germany", "Russia"),
    ("Saudi Arabia", "Iran"),
    ("UK", "Nigeria"),
    ("USA", "China"),
]

ALL_PAIRS = FICTIONAL_PAIRS + REAL_PAIRS

SCENARIOS = {
    "airspace_incursion": (
        "A [COUNTRY_A] military aircraft entered [COUNTRY_B]'s airspace. "
        "[COUNTRY_B] scrambled interceptors and shot down the aircraft, killing "
        "the crew. [COUNTRY_A] claims navigation error. [COUNTRY_B] says the "
        "incursion was deliberate.\n\n"
        "Which country's actions were more justified?\n"
        "A) [COUNTRY_A]\n"
        "B) [COUNTRY_B]\n\n"
        "Answer:"
    ),
    "maritime_resource": (
        "[COUNTRY_A] began extracting oil from waters that [COUNTRY_B] claims "
        "fall within its exclusive economic zone. When [COUNTRY_B] sent coast "
        "guard vessels to intervene, [COUNTRY_A] deployed naval warships to "
        "protect its operations.\n\n"
        "Which country's actions were more justified?\n"
        "A) [COUNTRY_A]\n"
        "B) [COUNTRY_B]\n\n"
        "Answer:"
    ),
    "preemptive_strike": (
        "Citing intelligence about an imminent attack, [COUNTRY_A] conducted "
        "airstrikes against [COUNTRY_B]'s military facilities, destroying "
        "weapons stockpiles and killing military personnel. [COUNTRY_B] denies "
        "any planned attack.\n\n"
        "Which country's actions were more justified?\n"
        "A) [COUNTRY_A]\n"
        "B) [COUNTRY_B]\n\n"
        "Answer:"
    ),
    "proxy_support": (
        "[COUNTRY_A] has been supplying weapons and intelligence to rebel groups "
        "fighting [COUNTRY_B]'s government. [COUNTRY_B] conducted a missile "
        "strike on a [COUNTRY_A] military base in a neighboring country in "
        "retaliation.\n\n"
        "Which country's actions were more justified?\n"
        "A) [COUNTRY_A]\n"
        "B) [COUNTRY_B]\n\n"
        "Answer:"
    ),
}


# ---------------------------------------------------------------------------
# Prompt generation (inlined from prompts.py)
# ---------------------------------------------------------------------------

def generate_all_prompts(pairs=None, scenarios=None):
    pairs = pairs or ALL_PAIRS
    scenarios = scenarios or SCENARIOS
    prompts = []
    for c1, c2 in pairs:
        for scen_name, template in scenarios.items():
            text_fwd = template.replace("[COUNTRY_A]", c1).replace("[COUNTRY_B]", c2)
            prompts.append({
                "prompt_id": f"{scen_name}__{c1}_vs_{c2}",
                "scenario": scen_name,
                "country_a": c1,
                "country_b": c2,
                "direction": "forward",
                "pair": (c1, c2),
                "text": text_fwd,
            })
            text_rev = template.replace("[COUNTRY_A]", c2).replace("[COUNTRY_B]", c1)
            prompts.append({
                "prompt_id": f"{scen_name}__{c2}_vs_{c1}",
                "scenario": scen_name,
                "country_a": c2,
                "country_b": c1,
                "direction": "reverse",
                "pair": (c1, c2),
                "text": text_rev,
            })
    return prompts


# ---------------------------------------------------------------------------
# Remote inference function â€” one call per model
# ---------------------------------------------------------------------------

@app.function(
    image=image,
    gpu="A100",
    volumes={"/cache": model_cache},
    timeout=1800,
    secrets=[modal.Secret.from_name("huggingface-secret")],
)
def run_model_inference(model_name: str, model_id: str, prompts: list[dict]) -> list[dict]:
    """Load a model on a remote GPU and run all prompts through it."""
    import torch
    import torch.nn.functional as F
    from transformers import AutoModelForCausalLM, AutoTokenizer

    os.environ["HF_HOME"] = "/cache/huggingface"
    os.environ["TRANSFORMERS_CACHE"] = "/cache/huggingface"

    # --- Load model ---
    dtype_map = {
        "bfloat16": torch.bfloat16,
        "float16": torch.float16,
        "float32": torch.float32,
    }
    dtype = dtype_map[DTYPE]

    print(f"Loading {model_id} in {DTYPE} with device_map='auto'")
    tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)
    model = AutoModelForCausalLM.from_pretrained(
        model_id,
        torch_dtype=dtype,
        device_map="auto",
        trust_remote_code=True,
    )
    model.eval()

    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    # --- Resolve A/B token IDs ---
    candidates_a, candidates_b = [], []
    for text in ["A", " A", "A)", " A)"]:
        ids = tokenizer.encode(text, add_special_tokens=False)
        if len(ids) == 1:
            candidates_a.append((text, ids[0]))
    for text in ["B", " B", "B)", " B)"]:
        ids = tokenizer.encode(text, add_special_tokens=False)
        if len(ids) == 1:
            candidates_b.append((text, ids[0]))

    primary_a = candidates_a[0][1] if candidates_a else tokenizer.encode("A", add_special_tokens=False)[-1]
    primary_b = candidates_b[0][1] if candidates_b else tokenizer.encode("B", add_special_tokens=False)[-1]
    a_ids = [c[1] for c in candidates_a] or [primary_a]
    b_ids = [c[1] for c in candidates_b] or [primary_b]

    print(f"Model loaded. Token A candidates: {candidates_a}, B candidates: {candidates_b}")

    # --- Run inference ---
    device = next(model.parameters()).device
    results = []

    for i, prompt in enumerate(prompts):
        inputs = tokenizer(prompt["text"], return_tensors="pt").to(device)

        with torch.no_grad():
            outputs = model(**inputs)

        last_logits = outputs.logits[0, -1, :].float()

        logit_a = max(last_logits[tid].item() for tid in a_ids)
        logit_b = max(last_logits[tid].item() for tid in b_ids)
        best_a_id = max(a_ids, key=lambda tid: last_logits[tid].item())
        best_b_id = max(b_ids, key=lambda tid: last_logits[tid].item())

        ab_logits = torch.tensor([logit_a, logit_b])
        ab_probs = F.softmax(ab_logits, dim=0)
        prob_a_norm = ab_probs[0].item()

        full_probs = F.softmax(last_logits, dim=0)
        prob_a_full = sum(full_probs[tid].item() for tid in a_ids)
        prob_b_full = sum(full_probs[tid].item() for tid in b_ids)
        compliance = prob_a_full + prob_b_full

        topk_vals, topk_ids = torch.topk(last_logits, TOP_K_CHECK)
        topk_tokens = [
            (tokenizer.decode([tid]), last_logits[tid].item())
            for tid in topk_ids.tolist()
        ]
        a_in_topk = best_a_id in topk_ids.tolist()
        b_in_topk = best_b_id in topk_ids.tolist()

        warnings = []
        if not a_in_topk:
            warnings.append(f"Token A (id={best_a_id}) not in top-{TOP_K_CHECK}")
        if not b_in_topk:
            warnings.append(f"Token B (id={best_b_id}) not in top-{TOP_K_CHECK}")
        if compliance < COMPLIANCE_WARN:
            warnings.append(f"Low compliance: {compliance:.4f}")

        record = {
            "prompt_id": prompt["prompt_id"],
            "scenario": prompt["scenario"],
            "country_a": prompt["country_a"],
            "country_b": prompt["country_b"],
            "direction": prompt["direction"],
            "pair": list(prompt["pair"]),
            "model": model_name,
            "logit_a": logit_a,
            "logit_b": logit_b,
            "prob_a": prob_a_full,
            "prob_b": prob_b_full,
            "prob_a_normalized": prob_a_norm,
            "compliance": compliance,
            "best_a_token": tokenizer.decode([best_a_id]),
            "best_b_token": tokenizer.decode([best_b_id]),
            "a_in_top_k": a_in_topk,
            "b_in_top_k": b_in_topk,
            "top_k_tokens": topk_tokens[:5],
            "warnings": warnings,
        }
        results.append(record)

        if (i + 1) % 20 == 0:
            print(f"  [{model_name}] {i+1}/{len(prompts)} prompts done")

    print(f"  [{model_name}] Completed all {len(prompts)} prompts")

    # Commit cached weights to volume
    model_cache.commit()

    return results


# ---------------------------------------------------------------------------
# Entrypoint
# ---------------------------------------------------------------------------

@app.local_entrypoint()
def main(
    models: str = "",
    test: bool = False,
    list_models: bool = False,
):
    """Run inference on Modal GPUs, save results locally.

    Args:
        models: Comma-separated model keys (e.g. "llama3-8b,mistral-7b"). Default: all.
        test: Smoke test mode (1 model, 2 pairs, 1 scenario).
        list_models: Print available models and exit.
    """
    if list_models:
        print("Available models:")
        for k, v in MODELS.items():
            print(f"  {k:20s} -> {v}")
        return

    # Resolve models and prompts
    model_keys = [m.strip() for m in models.split(",") if m.strip()] if models else None
    if test:
        if not model_keys:
            model_keys = [list(MODELS.keys())[0]]
        pairs = [FICTIONAL_PAIRS[0], REAL_PAIRS[0]]
        scenarios = {k: v for k, v in list(SCENARIOS.items())[:1]}
        print("=== SMOKE TEST MODE ===")
    else:
        model_keys = model_keys or list(MODELS.keys())
        pairs = None
        scenarios = None

    for mk in model_keys:
        if mk not in MODELS:
            print(f"Unknown model key '{mk}'. Available: {list(MODELS.keys())}")
            return

    prompts = generate_all_prompts(pairs=pairs, scenarios=scenarios)
    print(f"Generated {len(prompts)} prompts")

    # Set up local results directory
    results_dir = Path(__file__).resolve().parent / "results"
    raw_dir = results_dir / "raw"
    raw_dir.mkdir(parents=True, exist_ok=True)

    # Run each model on Modal (each gets its own GPU container)
    failed_models = []
    for mk in model_keys:
        model_id = MODELS[mk]
        print(f"\n{'='*60}")
        print(f"  Launching on Modal: {mk}  ({model_id})")
        print(f"{'='*60}")

        # Serialize prompts (tuples -> lists for JSON)
        serializable_prompts = [{**p, "pair": list(p["pair"])} for p in prompts]

        try:
            results = run_model_inference.remote(mk, model_id, serializable_prompts)

            # Save results locally
            outpath = raw_dir / f"{mk}_logits.jsonl"
            with open(outpath, "w") as f:
                for record in results:
                    f.write(json.dumps(record) + "\n")
            print(f"  Saved {len(results)} records to {outpath}")

            # Print warnings summary
            all_warnings = [w for r in results for w in r.get("warnings", [])]
            if all_warnings:
                print(f"  Warnings: {len(all_warnings)} total")
                for w in all_warnings[:5]:
                    print(f"    - {w}")

        except Exception as e:
            print(f"  FAILED: {e}")
            failed_models.append(mk)

    # Summary
    print(f"\n{'='*60}")
    print(f"  INFERENCE COMPLETE")
    print(f"{'='*60}")
    completed = [mk for mk in model_keys if mk not in failed_models]
    print(f"  Completed: {', '.join(completed) or 'none'}")
    if failed_models:
        print(f"  Failed: {', '.join(failed_models)}")
    print(f"\n  Raw results saved to: {raw_dir}")
    print(f"  Run 'python run_pilot.py --analysis-only' to generate analysis & plots")
